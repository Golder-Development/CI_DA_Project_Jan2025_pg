{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Individual Formative assignment January 2025**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Design and implement an ETL pipeline using Python.\n",
        "* Visualise data using Matplotlib, Seaborn, and Plotly.\n",
        "* Adhere to the key milestones and deliverables.\n",
        "* Maximise future maintainability through documentation, code structure, and organisation.\n",
        "* Document and present the project process and outcomes.\n",
        "* Demonstrate and document the development process through a version control system such as GitHub.\n",
        "\n",
        "Based of the Retail Sales Data Analysis Project Example.  \n",
        "\n",
        "* Data analysis goals:  \n",
        "Analyse retail sales data to identify trends, insights, and the impact of promotional markdowns on sales.  \n",
        "Provide comprehensive, visually appealing sales reports and insights to assist in strategic decision-making.\n",
        "\n",
        "Context:  \n",
        "\n",
        "The challenge involves making decisions based on limited historical data, particularly around holidays and promotional events.  \n",
        "The dataset includes historical sales data for 45 stores in different regions, with details about store types, sizes, and promotional markdowns.  \n",
        "\n",
        "*  Potential features to include:  \n",
        "ETL Pipeline:  \n",
        "    Extract: Load data from the Excel sheets (Stores, Features, Sales).  \n",
        "    Transform: Clean the data, handle missing values, and create new features such as sales differences between holiday and non-holiday weeks.  \n",
        "    Load: Store the transformed data in a format suitable for analysis (e.g., a cleaned DataFrame).  \n",
        "Data Visualisation:  \n",
        "    Descriptive Statistics: Display basic statistics such as average sales per store and department.  \n",
        "    Trend Analysis: Plot sales trends over time for different stores and departments.  \n",
        "    Impact Analysis: Visualise the impact of markdowns on sales during holidays versus non-holiday periods.  \n",
        "    Comparative Analysis: Compare sales performance across different stores and regions.  \n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Data source: https://www.kaggle.com/datasets/manjeetsingh/retaildataset \n",
        "\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* This notebook will be the combined output for the 2 day assignment\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* If you have any additional comments that don't fit in the previous bullets, please state them here. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/WORKSPACE/CI_DA_PROJECT_JAN2025_PG'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[226], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/WORKSPACE/CI_DA_PROJECT_JAN2025_PG\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/WORKSPACE/CI_DA_PROJECT_JAN2025_PG'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir('/WORKSPACE/CI_DA_PROJECT_JAN2025_PG')\n",
        "runtime.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/'"
            ]
          },
          "execution_count": 224,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/'"
            ]
          },
          "execution_count": 225,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# ETL - Step 1 data load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 1 - setup and import relevant modules\n",
        "\n",
        "In order to undertake the required tasks the following python modules will be loaded into the Jupyter notebook.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import math\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load ETL Modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import math\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Original Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'sales_data-set.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[222], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sales_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msales_data-set.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m stores_df\u001b[38;5;241m=\u001b[39m  pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstores_data-set.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m features_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeatures_data_set.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.9/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sales_data-set.csv'"
          ]
        }
      ],
      "source": [
        "sales_df = pd.read_csv('sales_data-set.csv')\n",
        "stores_df=  pd.read_csv('stores_data-set.csv')\n",
        "features_df = pd.read_csv('Features_data_set.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Starting with sales_df\n",
        "\n",
        "Review data and enhance it for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Headline data for \n",
        "Columns = ['Date', 'Dept', 'Store', 'Weekly_Sales', 'IsHoliday']\n",
        "\n",
        "sales_sum_df = pd.DataFrame({\n",
        "    'DataType': sales_df[Columns].dtypes,\n",
        "    'UniqueValues': sales_df[Columns].nunique(),\n",
        "    'RowsWithData': len(sales_df[Columns])-sales_df[Columns].isnull().sum(),\n",
        "    'NullValues':sales_df[Columns].isnull().sum(),\n",
        "    'SkewScore':sales_df[Columns].skew(),\n",
        "    'Kurtosis':sales_df[Columns].kurtosis()\n",
        "})\n",
        "sales_sum_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sales_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measures - Sales_data-set.csv\n",
        "\n",
        "This data set contain the following data elements, there are no Nulls in the data so there is no need for cleansing of Null.  \n",
        "The date field is not in date format = this needs to be changed.  \n",
        "\n",
        "sales_data-set.csv  \n",
        "    Date - Start/end date of week the data is relevant to  \n",
        "    Store - The store number the data is relevant to - 45 unique stores  \n",
        "    Dept - The Department within the store that the data is relevant to - 73 unique departments  \n",
        "    Weekly_sales - Total Weekly Sales for the department and store  \n",
        "    Is_Holiday = True/False flag that week contained holiday dates  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change date to date format and create year/month and week fields and YearMonth YearWeek\n",
        "# convert Received date to Date Format\n",
        "sales_df['Date'] = pd.to_datetime(sales_df['Date'], format='%d/%m/%Y')\n",
        "# Append Year column\n",
        "sales_df['SalesYear'] = round(sales_df['Date'].dt.year)\n",
        "# Append Month column \n",
        "sales_df['SalesMonth'] = round(sales_df['Date'].dt.month)\n",
        "# Append Week column\n",
        "sales_df['SalesWeek'] = round(sales_df['Date'].dt.week)\n",
        "# Create YearMonth Column\n",
        "sales_df['SalesYearMonth'] = round(sales_df['SalesYear']*100 +sales_df['SalesMonth'])\n",
        "# Create YearWeek Column\n",
        "sales_df['SalesYearWeek'] = round(sales_df['SalesYear']*100 +sales_df['SalesWeek'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Measures - Sales_data-set.csv\n",
        "\n",
        "Measures now available  \n",
        "\n",
        "sales_data-set.csv  \n",
        "    Date - Start/end date of week the data is relevant to.  \n",
        "    Store - The store number the data is relevant to - 45 unique stores.  \n",
        "    Dept - The Department within the store that the data is relevant to - 73 unique departments.  \n",
        "    Weekly_sales - Total Weekly Sales for the department and store.  \n",
        "    Is_Holiday = True/False flag that week contained holiday dates.  \n",
        "    SalesYear = Year the sale was made.  \n",
        "    SalesMonth - Month the sale was made.  \n",
        "    SalesWeek - Week the sale was made.  \n",
        "    SalesYearMonth - Year and Month the sale was made.  \n",
        "    SalesYearWeek - Year and Week the sale was made.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sales_df.query('SalesWeek <= 4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(16, 8))\n",
        "\n",
        "# First graph: Weekly sales over time\n",
        "sales_df.groupby(['Store', 'Date' ]).Weekly_Sales.sum().unstack(level=0).plot(\n",
        "    kind='line',\n",
        "    linestyle='solid',\n",
        "    legend=False,\n",
        "    ax=axes[0, 0]\n",
        ")\n",
        "axes[0, 0].set_title('Sales per Week')\n",
        "axes[0, 0].set_xlabel('Date')\n",
        "axes[0, 0].set_ylabel('Weekly Sales')\n",
        "axes[0, 0].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "\n",
        "\n",
        "# Second graph: Total sales per week aggregated by SalesWeek\n",
        "sales_df.groupby(['SalesYear', 'SalesWeek']).Weekly_Sales.sum().unstack(level=0).plot(\n",
        "    kind='line',\n",
        "    linestyle='solid',\n",
        "    legend=True,\n",
        "    ax=axes[0, 1]\n",
        ")\n",
        "axes[0, 1].set_title('Total Sales per Week: 2010 and 2011')\n",
        "axes[0, 1].set_xlabel('Sales Week')\n",
        "axes[0, 1].set_ylabel('Total Weekly Sales')\n",
        "axes[0, 1].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "axes[0, 1].xaxis.set_major_formatter(ticker.EngFormatter())\n",
        "\n",
        "# Third graph: Mean sales per week aggregated by SalesWeek\n",
        "sales_df.groupby(['SalesYear', 'SalesWeek']).Weekly_Sales.mean().unstack(level=0).plot(\n",
        "    kind='line',\n",
        "    linestyle='solid',\n",
        "    legend=True,\n",
        "    ax=axes[1, 0]\n",
        ")\n",
        "axes[1, 0].set_title('Mean Sales per Week per Department')\n",
        "axes[1, 0].set_xlabel('Sales Week')\n",
        "axes[1, 0].set_ylabel('Mean Department Weekly Sales')\n",
        "axes[1, 0].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "axes[1, 0].xaxis.set_major_formatter(ticker.EngFormatter())\n",
        "\n",
        "# Fourth graph: Year-on-year comparison of sales\n",
        "# Group data by year and month, then aggregate sales\n",
        "monthly_sales = sales_df.groupby(['SalesYear', 'SalesMonth']).Weekly_Sales.sum().unstack(level=0)\n",
        "\n",
        "# Plot one line per year\n",
        "monthly_sales.plot(\n",
        "    kind='line',\n",
        "    ax=axes[1, 1],\n",
        "    linestyle='solid',\n",
        "    marker='o'  # Optional: Adds markers to the line\n",
        ")\n",
        "\n",
        "# Update x-axis with month names\n",
        "axes[1, 1].set_xticks(range(1, 13))  # Months range from 1 to 12\n",
        "axes[1, 1].set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "axes[1, 1].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "\n",
        "# Add title and labels\n",
        "axes[1, 1].set_title('Year-on-Year Total Sales by Month')\n",
        "axes[1, 1].set_xlabel('Month')\n",
        "axes[1, 1].set_ylabel('Total Sales')\n",
        "\n",
        "# Add legend for years\n",
        "axes[1, 1].legend(title='Year', loc='upper left')\n",
        "\n",
        "# Fifth graph: Holiday vs Non Holiday\n",
        "# Group data by year and Holiday, then aggregate sales\n",
        "monthly_sales2 = sales_df.groupby(['IsHoliday', 'SalesYear']).Weekly_Sales.mean().unstack(level=0)\n",
        "\n",
        "monthly_sales2.plot(\n",
        "    kind='bar',\n",
        "    ax=axes[2,0],\n",
        "    color=['skyblue', 'orange'],  # Optional: Custom colors for bars\n",
        "    edgecolor='black'            # Adds a border to the bars\n",
        ")\n",
        "\n",
        "# Add title and labels\n",
        "axes[2,0].set_title('Average Weekly Sales by Year and Holiday Status')\n",
        "axes[2,0].set_xlabel('Sales Year')\n",
        "axes[2,0].set_ylabel('Average Weekly Sales')\n",
        "axes[2,0].legend(title='Holiday Week', loc='upper left')\n",
        "axes[2,0].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "axes[2,0].xaxis.set_major_formatter(ticker.EngFormatter())\n",
        "\n",
        "# Sixth graph: Count Departments\n",
        "# Group data by year and Holiday, then aggregate sales\n",
        "dept_per_store = sales_df.groupby('Store')['Dept'].nunique()\n",
        "stores_with_same_no_dept = dept_per_store.value_counts()\n",
        "stores_with_same_no_dept.sort_index().plot(\n",
        "    kind='bar',\n",
        "    x='Dept',\n",
        "    ax=axes[2,1],\n",
        "    legend=False,\n",
        "    edgecolor='black'            # Adds a border to the bars\n",
        ")\n",
        "\n",
        "# Add title and labels\n",
        "axes[2,1].set_title('Departments by Store')\n",
        "axes[2,1].set_xlabel('Departments in Store')\n",
        "axes[2,1].set_ylabel('Count of Stores')\n",
        "\n",
        "# Display the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets see box and whisker charts by store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get unique values in RegEntity_Group\n",
        "unique_groups = sales_df['Store'].unique()\n",
        "\n",
        "# Number of plots per row\n",
        "plots_per_row = 6\n",
        "\n",
        "# Calculate the required number of rows\n",
        "num_rows = math.ceil(len(unique_groups) / plots_per_row)\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(nrows=num_rows, ncols=plots_per_row, figsize=(18, 3 * num_rows))\n",
        "axes = axes.flatten()  # Flatten the axes array for easier indexing\n",
        "\n",
        "# Loop through each unique value in RegEntity_Group and corresponding axes\n",
        "for idx, group in enumerate(unique_groups):\n",
        "    # Filter data for the current group\n",
        "    group_data = sales_df.loc[sales_df['Store'] == group, 'Weekly_Sales'].dropna()\n",
        "    \n",
        "    # Create a boxplot for the current group\n",
        "    axes[idx].boxplot(group_data, vert=True, patch_artist=True, labels=[group])\n",
        "    \n",
        "    # Add labels and title\n",
        "    axes[idx].set_title(f'Store {group}')\n",
        "    axes[idx].set_ylabel('Weekly_Sales')\n",
        "    axes[idx].set_xlabel('Store')\n",
        "    axes[idx].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "\n",
        "# Hide any unused axes\n",
        "for ax in axes[len(unique_groups):]:\n",
        "    ax.set_visible(False)\n",
        "\n",
        "# Adjust layout and show plot\n",
        "plt.yscale('log')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observations on Sales_data-set.csv\n",
        "\n",
        "The data does not contain 3 complete years, it runs from Jan 2010 to October 2012.  \n",
        "There is a peak in sales during December in 2010 and 2011.  \n",
        "There is a corresponding drop in sales in January of 2011 and 2012.  \n",
        "The 45 Stores appear to have relatively similar sales patterns across the year, although the Size of those sales does vary between store, this is evidently driven by a different factor. \n",
        " This observation is backed up by the variations shown in the Box and Whisker plots by store.  \n",
        " Stores received increased sales during holiday weeks on average than during non holiday weeks.  \n",
        " Number of deparments varies across Stores from 61 to 79, with most having 77 departments.  No stores have 65-71 departments. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lets look at the other data sets \n",
        "\n",
        "Lets see what the Features and Stores data sets add"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Headline data for \n",
        "print(features_df.describe())\n",
        "features_sum_df = pd.DataFrame({\n",
        "    'DataType': features_df.dtypes,\n",
        "    'UniqueValues': features_df.nunique(),\n",
        "    'RowsWithData': len(features_df)-features_df.isnull().sum(),\n",
        "    'NullValues': features_df.isnull().sum(),\n",
        "    'SkewScore': features_df.skew(),\n",
        "    'Kurtosis': features_df.kurtosis()\n",
        "})\n",
        "features_sum_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Measures - Features_data_set.csv\n",
        "\n",
        "Measures now available  \n",
        "\n",
        "Features_data_set.csv\n",
        "   CPI - Measure of average change over time in prices with 1980 being the base at 100 - 211 means prices are 111% greater than in 1980  \n",
        "   Date - Start/end date of week the data is relevant to.    \n",
        "   Fuel_Price - Average Price of fuel per gallon in week  \n",
        "   IsHoliday - True/False flag showing weeks with a holiday date.  \n",
        "   MarkDown1 - Value of discounts applied to a product during the period at the identified store  \n",
        "   MarkDown2 - Value of discounts applied to a product during the period at the identified store  \n",
        "   MarkDown3 - Value of discounts applied to a product during the period at the identified store  \n",
        "   MarkDown4 - Value of discounts applied to a product during the period at the identified store  \n",
        "   MarkDown5 - Value of discounts applied to a product during the period at the identified store  \n",
        "   Store - Store unique identifier  \n",
        "   Temperature - Average temperature during period  \n",
        "   Unemployment - Reported Unemployment rate for period.  \n",
        "\n",
        "There are Nulls in Markdown, CPI and Unemployment.\n",
        "\n",
        "CPI and Unemployment are Government measures - the FillNA fill forward method will be used for these  \n",
        "Markdown is a stated figure of discounts - NaN means no discounts applied - so NaN will be filled with 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Replace NaN in MarkDown as 0\n",
        "features_df['MarkDown1'] = features_df['MarkDown1'].fillna(0)\n",
        "features_df['MarkDown2'] = features_df['MarkDown2'].fillna(0)\n",
        "features_df['MarkDown3'] = features_df['MarkDown3'].fillna(0)\n",
        "features_df['MarkDown4'] = features_df['MarkDown4'].fillna(0)\n",
        "features_df['MarkDown5'] = features_df['MarkDown5'].fillna(0)\n",
        "features_df['MarkDownTotal']= features_df['MarkDown1']+features_df['MarkDown2']+features_df['MarkDown3']+features_df['MarkDown4']+features_df['MarkDown5']\n",
        "features_df['MarkDownData']= features_df['MarkDownTotal']>0\n",
        "# Use fillna forward on Unemployment and CPI\n",
        "features_df['CPI']=features_df['CPI'].fillna(method='ffill')\n",
        "features_df['Unemployment']=features_df['Unemployment'].fillna(method='ffill')\n",
        "\n",
        "# re run summary\n",
        "features_pc_sum_df = pd.DataFrame({\n",
        "    'DataType': features_df.dtypes,\n",
        "    'UniqueValues': features_df.nunique(),\n",
        "    'RowsWithData': len(features_df)-features_df.isnull().sum(),\n",
        "    'NullValues': features_df.isnull().sum(),\n",
        "    'SkewScore': features_df.skew(),\n",
        "    'Kurtosis': features_df.kurtosis()\n",
        "})\n",
        "features_pc_sum_df\n",
        "\n",
        "# "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Headline data for \n",
        "print(stores_df.describe())\n",
        "stores_sum_df = pd.DataFrame({\n",
        "    'DataType': stores_df.dtypes,\n",
        "    'UniqueValues': stores_df.nunique(),\n",
        "    'RowsWithData': len(stores_df)-stores_df.isnull().sum(),\n",
        "    'NullValues': stores_df.isnull().sum(),\n",
        "    'SkewScore': stores_df.skew(),\n",
        "    'Kurtosis': stores_df.kurtosis()\n",
        "})\n",
        "stores_sum_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stores_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Measures - Stores_data_set.csv\n",
        "\n",
        "Measures available  \n",
        "\n",
        "Stores_data_set.csv  \n",
        "    Store - Store unique Id   \n",
        "    Type - Store classification - choice of A, B or C  \n",
        "    Size - Store floor space in Sqft.  \n",
        "\n",
        "There are no Nulls that need cleaning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graphs from Stores - Code Corrected using ChatGpt\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 6))\n",
        "\n",
        "stores_df['Size'].plot(kind='hist', ax=axes[0], bins=20, edgecolor='black')\n",
        "axes[0].set_title('Distribution of Store Sizes')\n",
        "axes[0].set_xlabel('Size')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "stores_df['Type'].value_counts().plot(kind='bar', ax=axes[1], color='lightgreen', edgecolor='black')\n",
        "axes[1].set_title('Store Count by Type')\n",
        "axes[1].set_xlabel('Type')\n",
        "axes[1].set_ylabel('Count')\n",
        "\n",
        "stores_df.plot(kind='scatter', x='Type', y='Size', ax=axes[2], color='orange')\n",
        "axes[2].set_title('Size vs Type')\n",
        "axes[2].set_xlabel('Type')\n",
        "axes[2].set_ylabel('Size')\n",
        "axes[2].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enhance stores data with Count of Department and see if there is a relation between type and Number of Departments.\n",
        "\n",
        "Using the summary of department for store created earlier to produce the graph of count of stores by number of departments, the Stores_data-csv will be enhanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stores_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# merege the additional data on departments into data set so that it will be included further on\n",
        "stores_df=pd.merge(stores_df, dept_per_store, how='left', on=['Store'])\n",
        "\n",
        "# Graphs from Stores enhanced with number of departments\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 6))\n",
        "\n",
        "stores_df.plot(kind='scatter', x='Size', y='Dept')\n",
        "axes[0].set_title('Distribution of Store Sizes by number of departments')\n",
        "axes[0].set_xlabel('Size')\n",
        "axes[0].set_ylabel('Departments')\n",
        "\n",
        "stores_df.plot(kind='scatter', x='Type', y='Dept')\n",
        "axes[1].set_title('Departments by Type')\n",
        "axes[1].set_xlabel('Type')\n",
        "axes[1].set_ylabel('Departments')\n",
        "\n",
        "stores_df.plot(kind='scatter', x='Store', y='Dept', ax=axes[2], color='orange')\n",
        "axes[2].set_title('Size vs Type')\n",
        "axes[2].set_xlabel('Type')\n",
        "axes[2].set_ylabel('Size')\n",
        "axes[2].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observations Regarding the Stores Data\n",
        "\n",
        "There are significantly less Type C stores than those of A or B.  \n",
        "Store Type appears to be classified by Size with a few outliers in the A and B category.  \n",
        "Type potentially could be reclassfied and this presents a future option.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Append the Store Data into the Sales Data to enable groupby type and size also to look as sales by size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "enhancedsales_df=pd.merge(sales_df, stores_df, how='left', on=['Store'])\n",
        "enhancedsales_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add Sales/size of store to produce a normalised sales per sqft figure\n",
        "enhancedsales_df['SalesPerSqFt']=enhancedsales_df['Weekly_Sales']/enhancedsales_df['Size']\n",
        "enhancedsales_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create subplots\n",
        "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(16, 8))\n",
        "\n",
        "# First graph: Weekly sales over time\n",
        "enhancedsales_df.groupby(['Store', 'Date' ]).SalesPerSqFt.sum().unstack(level=0).plot(\n",
        "    kind='line',\n",
        "    linestyle='solid',\n",
        "    legend=False,\n",
        "    ax=axes[0, 0]\n",
        ")\n",
        "axes[0, 0].set_title('SalesPerSqFt per Week')\n",
        "axes[0, 0].set_xlabel('Date')\n",
        "axes[0, 0].set_ylabel('SalesPerSqFt')\n",
        "\n",
        "# Second graph: Total sales per week aggregated by SalesWeek\n",
        "enhancedsales_df.groupby(['SalesYear', 'SalesWeek']).SalesPerSqFt.sum().unstack(level=0).plot(\n",
        "    kind='line',\n",
        "    linestyle='solid',\n",
        "    legend=True,\n",
        "    ax=axes[0, 1]\n",
        ")\n",
        "axes[0, 1].set_title('SalesPerSqFt per Week: 2010 and 2011')\n",
        "axes[0, 1].set_xlabel('Sales Week')\n",
        "axes[0, 1].set_ylabel('Total SalesPerSqFt')\n",
        "\n",
        "# Third graph: Mean sales per week aggregated by SalesWeek\n",
        "enhancedsales_df.groupby(['SalesYear', 'SalesWeek']).SalesPerSqFt.mean().unstack(level=0).plot(\n",
        "    kind='line',\n",
        "    linestyle='solid',\n",
        "    legend=True,\n",
        "    ax=axes[1, 0]\n",
        ")\n",
        "axes[1, 0].set_title('Mean SalesPerSqFt per Department')\n",
        "axes[1, 0].set_xlabel('Sales Week')\n",
        "axes[1, 0].set_ylabel('Mean Department SalesPerSqFt Sales')\n",
        "\n",
        "# Fourth graph: Year-on-year comparison of sales\n",
        "# Group data by year and month, then aggregate sales\n",
        "monthly_sales = enhancedsales_df.groupby(['SalesYear', 'SalesMonth']).SalesPerSqFt.sum().unstack(level=0)\n",
        "\n",
        "# Plot one line per year\n",
        "monthly_sales.plot(\n",
        "    kind='line',\n",
        "    ax=axes[1, 1],\n",
        "    linestyle='solid',\n",
        "    marker='o'  # Optional: Adds markers to the line\n",
        ")\n",
        "\n",
        "# Update x-axis with month names\n",
        "axes[1, 1].set_xticks(range(1, 13))  # Months range from 1 to 12\n",
        "axes[1, 1].set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "\n",
        "# Add title and labels\n",
        "axes[1, 1].set_title('Year-on-Year SalesPerSqFt by Month')\n",
        "axes[1, 1].set_xlabel('Month')\n",
        "axes[1, 1].set_ylabel('SalesPerSqFt')\n",
        "\n",
        "# Add legend for years\n",
        "axes[1, 1].legend(title='Year', loc='upper left')\n",
        "\n",
        "# Fifth graph: Holiday vs Non Holiday\n",
        "# Group data by year and Holiday, then aggregate sales\n",
        "monthly_sales = enhancedsales_df.groupby(['IsHoliday', 'SalesYear']).SalesPerSqFt.mean().unstack(level=0)\n",
        "\n",
        "monthly_sales.plot(\n",
        "    kind='bar',\n",
        "    ax=axes[2,0],\n",
        "    color=['skyblue', 'orange'],  # Optional: Custom colors for bars\n",
        "    edgecolor='black'            # Adds a border to the bars\n",
        ")\n",
        "\n",
        "# Add title and labels\n",
        "axes[2,0].set_title('Average SalesPerSqFt by Year and Holiday Status')\n",
        "axes[2,0].set_xlabel('Sales Year')\n",
        "axes[2,0].set_ylabel('Average SalesPerSqFt')\n",
        "axes[2,0].legend(title='Holiday Week', loc='upper left')\n",
        "\n",
        "# Display the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observations on Sales Per Size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Append the Features Data into the Enhanced Sales Data to enable groupby type and size also to look as sales by size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change date on features_df to a dateformat.\n",
        "features_df['Date'] = pd.to_datetime(features_df['Date'], format='%d/%m/%Y')\n",
        "# merge enhanced and feature based on date and store\n",
        "enhancedsales_df=pd.merge(enhancedsales_df, features_df, how='left', on=['Store','Date'])\n",
        "enhancedsales_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#create a summary of sales per store per week\n",
        "# function_dictionary = {'OrderID':'count','Quantity':'mean'}\n",
        "salesaggregate = {'Weekly_Sales':'sum', 'SalesPerSqFt':'sum', 'Size': 'max', 'Temperature': 'max', 'Fuel_Price': 'max', 'Dept': 'count', 'Unemployment': 'max', 'IsHoliday_x': 'max', 'SalesYear': 'max', 'SalesWeek': 'max', 'SalesMonth': 'max', 'Type': 'first', 'CPI': 'max', 'MarkDownTotal': 'max', 'MarkDownData': 'max'}\n",
        "storesalessummary = enhancedsales_df.groupby(['Store','Date'], as_index=False).agg(salesaggregate).reset_index()\n",
        "\n",
        "\n",
        "storesalessummary['NonMarkedDownSales'] = storesalessummary['Weekly_Sales'] - storesalessummary['MarkDownTotal']\n",
        "storesalessummary['PctSalesMarkedDown'] = storesalessummary['MarkDownTotal']/storesalessummary['Weekly_Sales']\n",
        "storesalessummary['MD_PerSqFt'] = storesalessummary['MarkDownTotal']/storesalessummary['Size']\n",
        "storesalessummary['NMDS_PerSqFt'] = storesalessummary['NonMarkedDownSales']/ storesalessummary['Size']\n",
        "\n",
        "storesalessummary.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ReRun Box and Whisker per store\n",
        "\n",
        "# Get unique values in RegEntity_Group\n",
        "unique_groups = storesalessummary['Store'].unique()\n",
        "\n",
        "# Number of plots per row\n",
        "plots_per_row = 6\n",
        "\n",
        "# Calculate the required number of rows\n",
        "num_rows = math.ceil(len(unique_groups) / plots_per_row)\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(nrows=num_rows, ncols=plots_per_row, figsize=(18, 3 * num_rows))\n",
        "axes = axes.flatten()  # Flatten the axes array for easier indexing\n",
        "\n",
        "# Loop through each unique value in RegEntity_Group and corresponding axes\n",
        "for idx, group in enumerate(unique_groups):\n",
        "    # Filter data for the current group\n",
        "    group_data = storesalessummary.loc[storesalessummary['Store'] == group, 'SalesPerSqFt'].dropna()\n",
        "    \n",
        "    # Create a boxplot for the current group\n",
        "    axes[idx].boxplot(group_data, vert=True, patch_artist=True, labels=[group])\n",
        "    \n",
        "    # Add labels and title\n",
        "    axes[idx].set_title(f'Strore {group}')\n",
        "    axes[idx].set_ylabel('SalesPerSqFt')\n",
        "    axes[idx].set_xlabel('Store')\n",
        "\n",
        "# Hide any unused axes\n",
        "for ax in axes[len(unique_groups):]:\n",
        "    ax.set_visible(False)\n",
        "\n",
        "# Adjust layout and show plot\n",
        "plt.yscale('log')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observations on Sales By Store Type\n",
        "\n",
        "Unsuprisingly the large stores on average have higher weekly sales both during Holiday and Non Holiday weeks.  \n",
        "However Type C the smaller stores and Type B the Mid range stores - during non holiday weeks have higher Average Sales per Sq Ft than the larger Type A stores.  \n",
        "During Holiday periods Type B Stores have a similar Average sales per sq ft at Type A but their distributed sales per sq ft are generally higher.  \n",
        "Type C out performs both Type B and Type A in Sales Per Sq Ft during Holiday weeks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 6))\n",
        "\n",
        "Weekly_storesalessummary = pd.pivot_table(storesalessummary, index ='SalesWeek', columns = 'SalesYear', values = 'Weekly_Sales', aggfunc='mean')\n",
        "Weekly_storesalessummary.plot(ax=axes[0])\n",
        "axes[0].set_title('Avg Store Sales')\n",
        "axes[0].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "Weekly_storesalessummary2 = storesalessummary.query('MarkDownData == True').pivot_table(index ='SalesMonth', columns = 'SalesYear', values = 'NonMarkedDownSales', aggfunc='mean')\n",
        "Weekly_storesalessummary2.plot(ax=axes[1])\n",
        "axes[1].set_title('Avg Store Non Discounted Sales')\n",
        "axes[1].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "Weekly_storesalessummary3 = storesalessummary.query('MarkDownData == True').pivot_table(index ='SalesMonth', columns = 'SalesYear', values = 'MarkDownTotal', aggfunc='mean')\n",
        "Weekly_storesalessummary3.plot(ax=axes[2])\n",
        "axes[2].set_title('Avg Store Discounted Sales')\n",
        "axes[2].yaxis.set_major_formatter(ticker.EngFormatter())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create subplots\n",
        "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 10))\n",
        "\n",
        "# First plot\n",
        "sns.violinplot(\n",
        "    x='Type', y='Weekly_Sales', data=storesalessummary[['Weekly_Sales', 'Type']],\n",
        "    ax=axes[0, 0], scale='width'  # Assign the specific subplot axis\n",
        ")\n",
        "axes[0, 0].set_title('Comparing Store Sales by Type')\n",
        "axes[0, 0].set_ylim(-250000, 4250000)\n",
        "axes[0, 0].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        " \n",
        "\n",
        "# Second plot\n",
        "sns.violinplot(\n",
        "    x='Type', y='SalesPerSqFt', data=storesalessummary[['SalesPerSqFt', 'Type']],\n",
        "    ax=axes[1, 0], scale='width'  # Assign the specific subplot axis\n",
        ")\n",
        "axes[1, 0].set_title('Comparing Store SalesPerSqFt by Type')\n",
        "axes[1, 0].set_ylim(0, 33) \n",
        "axes[1, 0].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "\n",
        "# third plot = Weekly Sales for Holiday Weeks\n",
        "sns.violinplot(\n",
        "    x='Type', y='Weekly_Sales', data=storesalessummary.query('IsHoliday_x == True')[['Weekly_Sales', 'Type']],\n",
        "    ax=axes[0, 1], scale='width'  # Assign the specific subplot axis\n",
        ")\n",
        "axes[0, 1].set_title('Comparing Store Holiday Sales by Type')\n",
        "axes[0, 1].set_ylim(-250000, 4250000) \n",
        "axes[0, 1].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "\n",
        "# Forth plot = Weekly SalesPerSqFt for Holiday Weeks\n",
        "sns.violinplot(\n",
        "    x='Type', y='SalesPerSqFt', data=storesalessummary.query('IsHoliday_x == True')[['SalesPerSqFt', 'Type']],\n",
        "    ax=axes[1, 1], scale='width'  # Assign the specific subplot axis\n",
        ")\n",
        "axes[1, 1].set_title('Comparing Store Holiday SalesPerSqFt by Type')\n",
        "axes[1, 1].set_ylim(0, 33)\n",
        "axes[1, 1].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "\n",
        "# fifth plot = Weekly Sales for Non Holiday Weeks\n",
        "sns.violinplot(\n",
        "    x='Type', y='Weekly_Sales', data=storesalessummary.query('IsHoliday_x == False')[['Weekly_Sales', 'Type']],\n",
        "    ax=axes[0, 2], scale='width'  # Assign the specific subplot axis\n",
        ")\n",
        "axes[0, 2].set_title('Comparing Store Non Holiday Sales by Type')\n",
        "axes[0, 2].set_ylim(-250000, 4250000) \n",
        "axes[0, 2].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "\n",
        "# Six plot = Weekly SalesPerSqFt for Non Holiday Weeks\n",
        "sns.violinplot(\n",
        "    x='Type', y='SalesPerSqFt', data=storesalessummary.query('IsHoliday_x == False')[['SalesPerSqFt', 'Type']],\n",
        "    ax=axes[1, 2], scale='width'  # Assign the specific subplot axis\n",
        ")\n",
        "axes[1, 2].set_title('Comparing Store Non Holiday SalesPerSqFt by Type')\n",
        "axes[1, 2].set_ylim(0, 33)\n",
        "axes[1, 2].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Store and Department level performance information and Graphs.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the sales_df at weekly sales per department level produce a set of visualisations to enable store managers to review store specific performance by department over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate Delta fields for Weekly Sales both Year on Year and Week on Week, aslo create moving 13 week average.\n",
        "\n",
        "# Add rolling 4 and 13 week total sales\n",
        "sales_df['Dept_Roll_4wk_sales'] = sales_df.sort_values(['Store','Dept','Date'])['Weekly_Sales'].rolling(4).sum()\n",
        "sales_df['Dept_Roll_Qtr_sales'] = sales_df.sort_values(['Store','Dept','Date'])['Weekly_Sales'].rolling(13).sum()\n",
        "\n",
        "# week on week PCT change per store department\n",
        "sales_df['Dept_weekly_Growth_Rate'] = sales_df.sort_values(['Store','Dept','Date'])['Weekly_Sales'].pct_change(periods=1) * 100\n",
        "# Year on Year PCT Change per store department\n",
        "sales_df['Dept_yearly_Growth_Rate'] = sales_df.sort_values(['Store','Dept','Date'])['Weekly_Sales'].pct_change(periods=53) * 100\n",
        "# week on week PCT change per store department for 4 week rolling and 13 week rolling\n",
        "sales_df['Dept_4wk_roll_Growth_Rate'] = sales_df.sort_values(['Store','Dept','Date'])['Dept_Roll_4wk_sales'].pct_change(periods=1) * 100\n",
        "sales_df['Dept_13wk_roll_Growth_Rate'] = sales_df.sort_values(['Store','Dept','Date'])['Dept_Roll_Qtr_sales'].pct_change(periods=1) * 100\n",
        "\n",
        "sales_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get unique values in RegEntity_Group\n",
        "unique_groups = storesalessummary['Type'].unique()\n",
        "\n",
        "# Number of plots per row\n",
        "plots_per_row = 3\n",
        "\n",
        "# Calculate the required number of rows\n",
        "num_rows = math.ceil(len(unique_groups) / plots_per_row)\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(nrows=num_rows, ncols=plots_per_row, figsize=(18, 6 * num_rows))\n",
        "axes = axes.flatten()  # Flatten the axes array for easier indexing\n",
        "\n",
        "# Loop through each unique value in RegEntity_Group and corresponding axes\n",
        "for idx, group in enumerate(unique_groups):\n",
        "    # Filter data for the current group\n",
        "    group_data = storesalessummary.loc[storesalessummary['Type'] == group, 'Weekly_Sales'].dropna()\n",
        "    \n",
        "    # Create a boxplot for the current group\n",
        "    axes[idx].boxplot(group_data, vert=True, patch_artist=True, labels=[group])\n",
        "    \n",
        "    # Add labels and title\n",
        "    axes[idx].set_title(f'Boxplot of Value for {group}')\n",
        "    axes[idx].set_ylabel('Weekly_Sales')\n",
        "    axes[idx].set_xlabel('Store Type')\n",
        "    axes[idx].set_ylim(-250000 , 4250000)\n",
        "\n",
        "# Hide any unused axes\n",
        "for ax in axes[len(unique_groups):]:\n",
        "    ax.set_visible(False)\n",
        "\n",
        "# Adjust layout and show plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Get unique values in RegEntity_Group\n",
        "unique_groups = storesalessummary['Type'].unique()\n",
        "\n",
        "# Number of plots per row\n",
        "plots_per_row = 3\n",
        "\n",
        "# Calculate the required number of rows\n",
        "num_rows = math.ceil(len(unique_groups) / plots_per_row)\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(nrows=num_rows, ncols=plots_per_row, figsize=(18, 6 * num_rows))\n",
        "axes = axes.flatten()  # Flatten the axes array for easier indexing\n",
        "\n",
        "# Loop through each unique value in RegEntity_Group and corresponding axes\n",
        "for idx, group in enumerate(unique_groups):\n",
        "    # Filter data for the current group\n",
        "    group_data = storesalessummary.loc[storesalessummary['Type'] == group, 'MarkDownTotal'].dropna()\n",
        "    \n",
        "    # Create a boxplot for the current group\n",
        "    axes[idx].boxplot(group_data, vert=True, patch_artist=True, labels=[group])\n",
        "    \n",
        "    # Add labels and title\n",
        "    axes[idx].set_title(f'Boxplot of Value for {group}')\n",
        "    axes[idx].set_ylabel('MarkDownTotal')\n",
        "    axes[idx].set_xlabel('Store Type')\n",
        "\n",
        "# Hide any unused axes\n",
        "for ax in axes[len(unique_groups):]:\n",
        "    ax.set_visible(False)\n",
        "\n",
        "# Adjust layout and show plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create subplots\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18, 18))\n",
        "\n",
        "storesalessummary.query('MarkDownTotal >0').plot(kind='scatter', x='Weekly_Sales', y='MarkDownTotal', c='Store', colormap='viridis', ax=axes[0, 0])\n",
        "axes[0, 0].set_title('Weekly Sales vs Discount value')\n",
        "axes[0, 0].set_ylabel('Total Discount Applied')\n",
        "axes[0, 0].set_xlabel('Total Weekly Sales')\n",
        "axes[0, 0].xaxis.set_major_formatter(ticker.EngFormatter())\n",
        "axes[0, 0].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "\n",
        "storesalessummary.query('MarkDownTotal >0').plot(kind='scatter', x='Weekly_Sales', y='PctSalesMarkedDown', c='Store', colormap='viridis', ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Weekly Sales vs % of Sales discounted')\n",
        "axes[0, 1].set_ylabel('Percentage Discounted')\n",
        "axes[0, 1].set_xlabel('Total Weekly Sales')\n",
        "axes[0, 1].xaxis.set_major_formatter(ticker.EngFormatter())\n",
        "axes[0, 1].yaxis.set_major_formatter(ticker.PercentFormatter(xmax =1))\n",
        "\n",
        "storesalessummary.query('MarkDownTotal >0').plot(kind='scatter', x='MarkDownTotal', y='PctSalesMarkedDown', c='Store', colormap='viridis', ax=axes[0, 2])\n",
        "axes[0, 2].set_title('Discount Sales vs % of Sales Discounted')\n",
        "axes[0, 2].set_ylabel('Percentage Discounted')\n",
        "axes[0, 2].set_xlabel('Total Discounted Sales')\n",
        "axes[0, 2].xaxis.set_major_formatter(ticker.EngFormatter())\n",
        "axes[0, 2].yaxis.set_major_formatter(ticker.PercentFormatter(xmax =1))\n",
        "\n",
        "storesalessummary.query('IsHoliday_x == True').plot(kind='scatter', x='Weekly_Sales', y='MarkDownTotal', c='PctSalesMarkedDown', colormap='viridis', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Weekly Sales vs Discount value for Holiday Weeks')\n",
        "axes[1, 0].set_ylabel('Total Discount Applied')\n",
        "axes[1, 0].set_xlabel('Total Weekly Sales')\n",
        "axes[1, 0].xaxis.set_major_formatter(ticker.EngFormatter())\n",
        "axes[1, 0].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "\n",
        "storesalessummary.query('IsHoliday_x == True').plot(kind='scatter', x='Weekly_Sales', y='PctSalesMarkedDown', c='MarkDownTotal', colormap='viridis', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Weekly Sales vs % of Sales Discounted holiday weeks')\n",
        "axes[1, 1].set_ylabel('Percentage Discounted')\n",
        "axes[1, 1].set_xlabel('Total Weekly Sales')\n",
        "axes[1, 1].xaxis.set_major_formatter(ticker.EngFormatter())\n",
        "axes[1, 1].yaxis.set_major_formatter(ticker.PercentFormatter(xmax =1))\n",
        "\n",
        "\n",
        "storesalessummary.query('IsHoliday_x == True').plot(kind='scatter', x='MarkDownTotal', y='PctSalesMarkedDown', c='Weekly_Sales', colormap='viridis', ax=axes[1, 2])\n",
        "axes[1, 2].set_title('Discount Sales vs % of Sales Discounted holiday weeks')\n",
        "axes[1, 2].set_ylabel('Percentage Discounted')\n",
        "axes[1, 2].set_xlabel('Total Discounted Sales')\n",
        "axes[1, 2].xaxis.set_major_formatter(ticker.EngFormatter())\n",
        "axes[1, 2].yaxis.set_major_formatter(ticker.PercentFormatter(xmax =1))\n",
        "\n",
        "storesalessummary.query('IsHoliday_x == False').plot(kind='scatter', x='Weekly_Sales', y='MarkDownTotal', c='PctSalesMarkedDown', colormap='viridis', ax=axes[2, 0])\n",
        "axes[2, 0].set_title('Weekly Sales vs Discount value Non Holiday weeks')\n",
        "axes[2, 0].set_ylabel('Total Discount Applied')\n",
        "axes[2, 0].set_xlabel('Total Weekly Sales')\n",
        "axes[2, 0].xaxis.set_major_formatter(ticker.EngFormatter())\n",
        "axes[2, 0].yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "\n",
        "storesalessummary.query('IsHoliday_x == False').plot(kind='scatter', x='Weekly_Sales', y='PctSalesMarkedDown', c='MarkDownTotal', colormap='viridis', ax=axes[2, 1])\n",
        "axes[2, 1].set_title('Weekly Sales vs % of Sales Marked down Non holiday week')\n",
        "axes[2, 1].set_ylabel('Percentage Discounted')\n",
        "axes[2, 1].set_xlabel('Total Weekly Sales')\n",
        "axes[2, 1].xaxis.set_major_formatter(ticker.EngFormatter())\n",
        "axes[2, 1].yaxis.set_major_formatter(ticker.PercentFormatter(xmax =1))\n",
        "\n",
        "storesalessummary.query('IsHoliday_x == False').plot(kind='scatter', x='MarkDownTotal', y='PctSalesMarkedDown', c='Weekly_Sales', colormap='viridis', ax=axes[2, 2])\n",
        "axes[2, 2].set_title('Discount Sales vs % of Sales Discounted Non holiday weeks')\n",
        "axes[2, 2].set_ylabel('Percentage Discounted')\n",
        "axes[2, 2].set_xlabel('Total Discounted Sales')\n",
        "axes[2, 2].xaxis.set_major_formatter(ticker.EngFormatter())\n",
        "axes[2, 2].yaxis.set_major_formatter(ticker.PercentFormatter(xmax =1))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Improve graphs by adding trend Line\n",
        "\n",
        "## To do this need to use Seaborn instead of Pandas.plot/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observations on relationships Between Weekly Sales, Total Discounted Sales and Percentage Discount Represented\n",
        "\n",
        "There appears to be a direct correlation between the size of the Percentage Discount applied and the volume of Discount Sales.  \n",
        "There is also a positive correlation between Total Sales and Percentage Discount applied.  This suggests that the greater the discount on sales the greater overall sales are.  \n",
        "\n",
        "Both of these observations are true for both Holiday and Non Holiday weeks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* You may add as many sections as you want, as long as it supports your project workflow.\n",
        "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Conclusion and Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In cases where you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
